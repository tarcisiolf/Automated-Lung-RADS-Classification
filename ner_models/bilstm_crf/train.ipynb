{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "from seqeval.scheme import IOB1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.lookup import KeyValueTensorInitializer, StaticHashTable\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, TimeDistributed, Dropout, Bidirectional, Dense, Layer, InputSpec\n",
    "from tensorflow_addons.text import crf_log_likelihood, viterbi_decode, crf_decode\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 2.10.0\n",
      "Tensorflow Addons Version 0.21.0\n",
      "Numpy Version 1.25.2\n",
      "Pandas Version 1.5.3\n",
      "Keras Version 2.10.0\n",
      "Sklearn Version 1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow Version\", tf. __version__)\n",
    "print(\"Tensorflow Addons Version\", tfa. __version__)\n",
    "print(\"Numpy Version\", np.__version__)\n",
    "print(\"Pandas Version\", pd.__version__)\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"Sklearn Version\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 2.13.1\n",
      "Tensorflow Addons Version 0.21.0\n",
      "Numpy Version 1.24.3\n",
      "Pandas Version 2.0.3\n",
      "Keras Version 2.13.1\n",
      "Sklearn Version 1.3.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorflow Version\", tf. __version__)\n",
    "print(\"Tensorflow Addons Version\", tfa. __version__)\n",
    "print(\"Numpy Version\", np.__version__)\n",
    "print(\"Pandas Version\", pd.__version__)\n",
    "print(\"Keras Version\", keras.__version__)\n",
    "print(\"Sklearn Version\", sklearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/tlf/Documents/mestrado/ner_models/data/df_tokens_labeled_iob.csv', encoding= 'utf-8', index_col=0)\n",
    "data_train = pd.read_csv('/home/tlf/Documents/mestrado/ner_models/data/df_train_tokens_labeled_iob.csv', encoding= 'utf-8', index_col=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuples(data):\n",
    "    iterator = zip(data[\"token\"].values.tolist(),\n",
    "                  data[\"iob_label\"].values.tolist())\n",
    "    return [(token, iob_label) for token, iob_label in iterator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data):\n",
    "  all_words = list(set(data[\"token\"].values))\n",
    "  all_tags = list(set(data[\"iob_label\"].values))\n",
    "\n",
    "  word2index = {word: idx + 2 for idx, word in enumerate(all_words)}\n",
    "\n",
    "  word2index[\"--UNKNOWN_WORD--\"] = 0\n",
    "\n",
    "  word2index[\"--PADDING--\"] = 1\n",
    "\n",
    "  index2word = {idx: word for word, idx in word2index.items()}\n",
    "\n",
    "  tag2index = {tag: idx + 1 for idx, tag in enumerate(all_tags)}\n",
    "  tag2index[\"--PADDING--\"] = 0\n",
    "\n",
    "  index2tag = {idx: word for word, idx in tag2index.items()}\n",
    "\n",
    "  return word2index, index2word, tag2index, index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(reports, word2index, tag2index):\n",
    "  contents = []\n",
    "  labels = []\n",
    "  for report in reports:\n",
    "    content = []\n",
    "    label = []\n",
    "    for i in range(len(report)):\n",
    "      token, iob_tag = report[i]\n",
    "      word_idx = word2index.get(token, 0)\n",
    "      tag_idx = tag2index.get(iob_tag, 0)\n",
    "      content.append(word_idx)\n",
    "      label.append(tag_idx)\n",
    "\n",
    "    contents.append(content)\n",
    "    labels.append(label)\n",
    "\n",
    "  \"\"\"\n",
    "  padding the array with max_sentence_size\n",
    "  pad_sequences(sequences, maxlen=None, dtype=\"int32\", padding=\"pre\", truncating=\"pre\", value=0.0,):\n",
    "  the maxlen argument if provided, or the length of the longest sequence in the list.\n",
    "  \"\"\"\n",
    "\n",
    "  max_sentence_size = 512\n",
    "  contents = tf.keras.preprocessing.sequence.pad_sequences(contents, maxlen=max_sentence_size, padding='post', value=1)\n",
    "  labels = tf.keras.preprocessing.sequence.pad_sequences(labels, maxlen=max_sentence_size, padding='post')\n",
    "\n",
    "\n",
    "  tag_size = len(tag2index)\n",
    "\n",
    "  labels_categorical = [tf.keras.utils.to_categorical(i, num_classes=tag_size) for i in labels]\n",
    "  labels_categorical = np.asarray(labels_categorical)\n",
    "\n",
    "  return contents, labels, labels_categorical, max_sentence_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['iob_label'].isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index, index2word, tag2index, index2tag = build_vocab(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = data_train.groupby(\"report\").apply(to_tuples).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences, tag_sequences, tag_sequences_categorical, max_len = tokenize(reports, word2index, tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(text_sequences[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer(input_dim, output_dim, input_length, mask_zero):\n",
    "    return Embedding(input_dim = input_dim, output_dim = output_dim, input_length = input_length, mask_zero = mask_zero)\n",
    "\n",
    "def bilstm_crf(maxlen, n_tags, lstm_units, embedding_dim, n_words, mask_zero, training = True):\n",
    "    \"\"\"\n",
    "    bilstm_crf - module to build BiLSTM-CRF model\n",
    "    Inputs:\n",
    "        - input_shape : tuple\n",
    "            Tensor shape of inputs, excluding batch size\n",
    "    Outputs:\n",
    "        - output : tensorflow.keras.outputs.output\n",
    "            BiLSTM-CRF output\n",
    "    \"\"\"\n",
    "    input = Input(shape = (maxlen,))\n",
    "    # Embedding layer\n",
    "    embeddings = embedding_layer(input_dim = n_words, output_dim = embedding_dim, input_length = maxlen, mask_zero = mask_zero)\n",
    "    output = embeddings(input)\n",
    "\n",
    "    # BiLSTM layer\n",
    "    output = Bidirectional(LSTM(units = lstm_units, return_sequences = True, recurrent_dropout = 0.1))(output)\n",
    "\n",
    "    # Dense layer\n",
    "    output = TimeDistributed(Dense(n_tags, activation = 'relu'))(output)\n",
    "\n",
    "    output = CRF(n_tags, name = 'crf_layer')(output)\n",
    "    return Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(Layer):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 sparse_target=True,\n",
    "                 transitions=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            output_dim (int): the number of labels to tag each temporal input.\n",
    "            sparse_target (bool): whether the the ground-truth label represented in one-hot.\n",
    "        Input shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        Output shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__(**kwargs)\n",
    "        self.output_dim = int(output_dim)\n",
    "        self.sparse_target = sparse_target\n",
    "        self.input_spec = InputSpec(min_ndim=3)\n",
    "        self.supports_masking = False\n",
    "        self.sequence_lengths = None\n",
    "        self.transitions = transitions\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        f_shape = tf.TensorShape(input_shape)\n",
    "        input_spec = InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n",
    "\n",
    "        if f_shape[-1] is None:\n",
    "            raise ValueError('The last dimension of the inputs to `CRF` '\n",
    "                             'should be defined. Found `None`.')\n",
    "        if f_shape[-1] != self.output_dim:\n",
    "            raise ValueError('The last dimension of the input shape must be equal to output'\n",
    "                             ' shape. Use a linear layer if needed.')\n",
    "        self.input_spec = input_spec\n",
    "        self.transitions = self.add_weight(name='transitions',\n",
    "                                           shape=[self.output_dim, self.output_dim],\n",
    "                                           initializer='glorot_uniform',\n",
    "                                           trainable=True)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Just pass the received mask from previous layer, to the next layer or\n",
    "        # manipulate it if this layer changes the shape of the input\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, sequence_lengths=None, training=None, **kwargs):\n",
    "        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
    "        if sequence_lengths is not None:\n",
    "            assert len(sequence_lengths.shape) == 2\n",
    "            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n",
    "            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n",
    "            assert seq_len_shape[1] == 1\n",
    "            self.sequence_lengths = K.flatten(sequence_lengths)\n",
    "        else:\n",
    "            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n",
    "                tf.shape(inputs)[1]\n",
    "            )\n",
    "\n",
    "        viterbi_sequence, _ = crf_decode(sequences,\n",
    "                                         self.transitions,\n",
    "                                         self.sequence_lengths)\n",
    "        output = K.one_hot(viterbi_sequence, self.output_dim)\n",
    "        return K.in_train_phase(sequences, output)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        def crf_loss(y_true, y_pred):\n",
    "            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
    "            log_likelihood, self.transitions = crf_log_likelihood(\n",
    "                y_pred,\n",
    "                tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n",
    "                self.sequence_lengths,\n",
    "                transition_params=self.transitions,\n",
    "            )\n",
    "            return tf.reduce_mean(-log_likelihood)\n",
    "        return crf_loss\n",
    "\n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        def viterbi_accuracy(y_true, y_pred):\n",
    "            # -1e10 to avoid zero at sum(mask)\n",
    "            mask = K.cast(\n",
    "                K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
    "            shape = tf.shape(y_pred)\n",
    "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
    "            if self.sparse_target:\n",
    "                y_true = K.argmax(y_true, 2)\n",
    "            y_pred = K.cast(y_pred, 'int32')\n",
    "            y_true = K.cast(y_true, 'int32')\n",
    "            corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "            return K.sum(corrects * mask) / K.sum(mask)\n",
    "        return viterbi_accuracy\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        tf.TensorShape(input_shape).assert_has_rank(3)\n",
    "        return input_shape[:2] + (self.output_dim,)\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CRF, self).get_config()\n",
    "        config.update({\n",
    "            'output_dim': self.output_dim,\n",
    "            'sparse_target': self.sparse_target,\n",
    "            'transitions': self.transitions.numpy()  # Convert the transitions to a NumPy array\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Since 'transitions' is a NumPy array, we need to convert it back to a tensor\n",
    "        transitions = tf.convert_to_tensor(config['transitions'])\n",
    "        # Create a new instance of CRF with the saved configuration\n",
    "        return cls(output_dim=config['output_dim'], sparse_target=config['sparse_target'], transitions=transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_conf_matrix(cm, tagid):\n",
    "    tag_name = index2tag[tagid]\n",
    "    print(\"Tag name: {}\".format(tag_name))\n",
    "    print(cm[tagid])\n",
    "    tn, fp, fn, tp = cm[tagid].ravel()\n",
    "    tag_acc = round( ((tp + tn) / (tn + fp + fn + tp)), 3)\n",
    "\n",
    "\n",
    "    print(\"Tag accuracy: {:.3f}\\n\".format(tag_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_word_test_sentences_and_tags(index2tag, index2word, X_test, y_test):\n",
    "\n",
    "    test_sentences= []\n",
    "    test_tags = []\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        aux_tag = []\n",
    "\n",
    "        report = \"\"\n",
    "        sentence = X_test[i]\n",
    "        tags = y_test[i]\n",
    "\n",
    "        for j in range(len(sentence)):\n",
    "            word = sentence[j]\n",
    "            tag = tags[j]\n",
    "            int_tag = np.where(tag == int(1))\n",
    "            if str(index2word[word]) != '--PADDING--':\n",
    "                report = report + \" \" + str(index2word[word])\n",
    "                aux_tag.append(index2tag[int(int_tag[0][0])])\n",
    "\n",
    "        test_sentences.append(report)\n",
    "        test_tags.append(aux_tag)\n",
    "\n",
    "    return test_sentences, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_df_model(train_sentences, train_tags):\n",
    "\n",
    "    train_df = pd.DataFrame(columns = ['report', 'word', 'tag'])\n",
    "\n",
    "    for i in range (len(train_sentences)):\n",
    "        sentence = train_sentences[i]\n",
    "        tags = train_tags[i]\n",
    "        \n",
    "        sentence = sentence.split()\n",
    "\n",
    "        if i < 10:\n",
    "            retval = \"\"\n",
    "            for w, t in zip(sentence, tags):\n",
    "                retval = retval + \"{:25}: {:10}\".format(w, t) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_0' + str(i)), 'word': w, 'tag' : t}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                train_df = pd.concat([train_df, df_new_row])\n",
    "\n",
    "        else:\n",
    "            retval = \"\"\n",
    "            for w, t in zip(sentence, tags):\n",
    "                retval = retval + \"{:25}: {:10}\".format(w, t) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_' + str(i)), 'word': w, 'tag' : t}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                train_df = pd.concat([train_df, df_new_row])\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_df_model_previous(test_sentences, test_tags, model, word2index, index2tag, MAX_SENTENCE):\n",
    "\n",
    "    test_df = pd.DataFrame(columns = ['report', 'word', 'tag', 'tag_pred'])\n",
    "\n",
    "    for i in range (len(test_sentences)):\n",
    "\n",
    "        tags = test_tags[i]\n",
    "        \n",
    "        sentence = sentence.split()\n",
    "        padded_sentence = sentence + [word2index[\"--PADDING--\"]] * (MAX_SENTENCE - len(sentence))\n",
    "        padded_sentence = [word2index.get(w, 0) for w in padded_sentence]\n",
    "\n",
    "        pred = model.predict(np.array([padded_sentence]))\n",
    "        pred = np.argmax(pred, axis=-1)\n",
    "\n",
    "        if i < 10:\n",
    "            retval = \"\"\n",
    "            for w, t, p in zip(sentence, tags, pred[0]):\n",
    "                retval = retval + \"{:25}: {:10} {:5}\".format(w, t, index2tag[p]) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_0' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                test_df = pd.concat([test_df, df_new_row])\n",
    "\n",
    "        else:\n",
    "            retval = \"\"\n",
    "            for w, t, p in zip(sentence, tags, pred[0]):\n",
    "                retval = retval + \"{:25}: {:10} {:5}\".format(w, t, index2tag[p]) + \"\\n\"\n",
    "                aux_dict = {'report': ('report_' + str(i)), 'word': w, 'tag' : t, 'tag_pred' : index2tag[p]}\n",
    "                df_new_row = pd.DataFrame([aux_dict])\n",
    "                test_df = pd.concat([test_df, df_new_row])\n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = text_sequences\n",
    "y_train = tag_sequences_categorical\n",
    "train_sentences, train_tags = number_to_word_test_sentences_and_tags(index2tag, index2word, text_sequences, tag_sequences_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_sequences_categorical[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_DROPOUT = 0.1\n",
    "MAX_EPOCHS = 10\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENSE_EMBEDDING_LIST = [25, 100, 300]\n",
    "LSTM_UNITS_LIST = [25, 50, 100]\n",
    "BATCH_SIZE_LIST = [4, 8, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(word2index)\n",
    "n_tags = len(tag2index)\n",
    "lstm_dropout = LSTM_DROPOUT\n",
    "epochs = MAX_EPOCHS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_00 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_00.summary()\n",
    "\n",
    "# compile model\n",
    "model_00.compile(optimizer = Adam(learning_rate = lr), loss = model_00.layers[-1].loss, metrics = model_00.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_00.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_01 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_01.summary()\n",
    "\n",
    "# compile model\n",
    "model_01.compile(optimizer = Adam(learning_rate = lr), loss = model_01.layers[-1].loss, metrics = model_01.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_01.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_02 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_02.summary()\n",
    "\n",
    "# compile model\n",
    "model_02.compile(optimizer = Adam(learning_rate = lr), loss = model_02.layers[-1].loss, metrics = model_02.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_02.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_03 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_03.summary()\n",
    "\n",
    "# compile model\n",
    "model_03.compile(optimizer = Adam(learning_rate = lr), loss = model_03.layers[-1].loss, metrics = model_03.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_03.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_04 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_04.summary()\n",
    "\n",
    "# compile model\n",
    "model_04.compile(optimizer = Adam(learning_rate = lr), loss = model_04.layers[-1].loss, metrics = model_04.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_04.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_05 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_05.summary()\n",
    "\n",
    "# compile model\n",
    "model_05.compile(optimizer = Adam(learning_rate = lr), loss = model_05.layers[-1].loss, metrics = model_05.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_05.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_06 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_06.summary()\n",
    "\n",
    "# compile model\n",
    "model_06.compile(optimizer = Adam(learning_rate = lr), loss = model_06.layers[-1].loss, metrics = model_06.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_06.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_07 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_07.summary()\n",
    "\n",
    "# compile model\n",
    "model_07.compile(optimizer = Adam(learning_rate = lr), loss = model_07.layers[-1].loss, metrics = model_07.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_07.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_08 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_08.summary()\n",
    "\n",
    "# compile model\n",
    "model_08.compile(optimizer = Adam(learning_rate = lr), loss = model_08.layers[-1].loss, metrics = model_08.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_08.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_09 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_09.summary()\n",
    "\n",
    "# compile model\n",
    "model_09.compile(optimizer = Adam(learning_rate = lr), loss = model_09.layers[-1].loss, metrics = model_09.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_09.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_10 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_10.summary()\n",
    "\n",
    "# compile model\n",
    "model_10.compile(optimizer = Adam(learning_rate = lr), loss = model_10.layers[-1].loss, metrics = model_10.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_10.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_11 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_11.summary()\n",
    "\n",
    "# compile model\n",
    "model_11.compile(optimizer = Adam(learning_rate = lr), loss = model_11.layers[-1].loss, metrics = model_11.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_11.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_12 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_12.summary()\n",
    "\n",
    "# compile model\n",
    "model_12.compile(optimizer = Adam(learning_rate = lr), loss = model_12.layers[-1].loss, metrics = model_12.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_12.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_13 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_13.summary()\n",
    "\n",
    "# compile model\n",
    "model_13.compile(optimizer = Adam(learning_rate = lr), loss = model_13.layers[-1].loss, metrics = model_13.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_13.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_14 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_14.summary()\n",
    "\n",
    "# compile model\n",
    "model_14.compile(optimizer = Adam(learning_rate = lr), loss = model_14.layers[-1].loss, metrics = model_14.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_14.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_15 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_15.summary()\n",
    "\n",
    "# compile model\n",
    "model_15.compile(optimizer = Adam(learning_rate = lr), loss = model_15.layers[-1].loss, metrics = model_15.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_15.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_16 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_16.summary()\n",
    "\n",
    "# compile model\n",
    "model_16.compile(optimizer = Adam(learning_rate = lr), loss = model_16.layers[-1].loss, metrics = model_16.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_16.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[1]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_17 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_17.summary()\n",
    "\n",
    "# compile model\n",
    "model_17.compile(optimizer = Adam(learning_rate = lr), loss = model_17.layers[-1].loss, metrics = model_17.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_17.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_18 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_18.summary()\n",
    "\n",
    "# compile model\n",
    "model_18.compile(optimizer = Adam(learning_rate = lr), loss = model_18.layers[-1].loss, metrics = model_18.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_18.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_19 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_19.summary()\n",
    "\n",
    "# compile model\n",
    "model_19.compile(optimizer = Adam(learning_rate = lr), loss = model_19.layers[-1].loss, metrics = model_19.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_19.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_20 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_20.summary()\n",
    "\n",
    "# compile model\n",
    "model_20.compile(optimizer = Adam(learning_rate = lr), loss = model_20.layers[-1].loss, metrics = model_20.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_20.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_21 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_21.summary()\n",
    "\n",
    "# compile model\n",
    "model_21.compile(optimizer = Adam(learning_rate = lr), loss = model_21.layers[-1].loss, metrics = model_21.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_21.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_22 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_22.summary()\n",
    "\n",
    "# compile model\n",
    "model_22.compile(optimizer = Adam(learning_rate = lr), loss = model_22.layers[-1].loss, metrics = model_22.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_22.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[1]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_23 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_23.summary()\n",
    "\n",
    "# compile model\n",
    "model_23.compile(optimizer = Adam(learning_rate = lr), loss = model_23.layers[-1].loss, metrics = model_23.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_23.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_24 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_24.summary()\n",
    "\n",
    "# compile model\n",
    "model_24.compile(optimizer = Adam(learning_rate = lr), loss = model_24.layers[-1].loss, metrics = model_24.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_24.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_25 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_25.summary()\n",
    "\n",
    "# compile model\n",
    "model_25.compile(optimizer = Adam(learning_rate = lr), loss = model_25.layers[-1].loss, metrics = model_25.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_25.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[2]\n",
    "lstm_unit = LSTM_UNITS_LIST[2]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model_26 = bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "model_26.summary()\n",
    "\n",
    "# compile model\n",
    "model_26.compile(optimizer = Adam(learning_rate = lr), loss = model_26.layers[-1].loss, metrics = model_26.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\"\"\"\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring\n",
    "one epoch finished and starting the next epoch. When training with input tensors such as\n",
    "TensorFlow data tensors, the default None is equal to the number of samples in your dataset\n",
    "divided by the batch size\n",
    "\"\"\"\n",
    "#steps_per_epoch = x_train_len / 4\n",
    "\n",
    "model_26.fit(X_train, y_train, epochs = epochs, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('/home/tlf/Documents/mestrado/ner_models/data/df_test_tokens_labeled_iob.csv', encoding= 'utf-8', index_col=0)\n",
    "reports = data_test.groupby(\"report\").apply(to_tuples).tolist()\n",
    "test_text_sequences, test_tag_sequences, test_tag_sequences_categorical, max_len = tokenize(reports, word2index, tag2index)\n",
    "\n",
    "X_test = test_text_sequences\n",
    "y_test = test_tag_sequences_categorical\n",
    "\n",
    "test_sentences, test_tags = number_to_word_test_sentences_and_tags(index2tag, index2word, test_text_sequences, test_tag_sequences_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_model_00 = result_df_model_previous(test_sentences, test_tags, model_00, word2index, index2tag, max_len)\n",
    "result_df_model_00.to_csv(\"result_df_model_00.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_01 = result_df_model_previous(test_sentences, test_tags, model_01, word2index, index2tag, max_len)\n",
    "result_df_model_01.to_csv(\"result_df_model_01.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_02 = result_df_model_previous(test_sentences, test_tags, model_02, word2index, index2tag, max_len)\n",
    "result_df_model_02.to_csv(\"result_df_model_02.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_03 = result_df_model_previous(test_sentences, test_tags, model_03, word2index, index2tag, max_len)\n",
    "result_df_model_03.to_csv(\"result_df_model_03.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_04 = result_df_model_previous(test_sentences, test_tags, model_04, word2index, index2tag, max_len)\n",
    "result_df_model_04.to_csv(\"result_df_model_04.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_05 = result_df_model_previous(test_sentences, test_tags, model_05, word2index, index2tag, max_len)\n",
    "result_df_model_05.to_csv(\"result_df_model_05.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_06 = result_df_model_previous(test_sentences, test_tags, model_06, word2index, index2tag, max_len)\n",
    "result_df_model_06.to_csv(\"result_df_model_06.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_07 = result_df_model_previous(test_sentences, test_tags, model_07, word2index, index2tag, max_len)\n",
    "result_df_model_07.to_csv(\"result_df_model_07.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_08 = result_df_model_previous(test_sentences, test_tags, model_08, word2index, index2tag, max_len)\n",
    "result_df_model_08.to_csv(\"result_df_model_08.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_09 = result_df_model_previous(test_sentences, test_tags, model_09, word2index, index2tag, max_len)\n",
    "result_df_model_09.to_csv(\"result_df_model_09.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_10 = result_df_model_previous(test_sentences, test_tags, model_10, word2index, index2tag, max_len)\n",
    "result_df_model_10.to_csv(\"result_df_model_10.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_11 = result_df_model_previous(test_sentences, test_tags, model_11, word2index, index2tag, max_len)\n",
    "result_df_model_11.to_csv(\"result_df_model_11.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_12 = result_df_model_previous(test_sentences, test_tags, model_12, word2index, index2tag, max_len)\n",
    "result_df_model_12.to_csv(\"result_df_model_12.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_13 = result_df_model_previous(test_sentences, test_tags, model_13, word2index, index2tag, max_len)\n",
    "result_df_model_13.to_csv(\"result_df_model_13.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_14 = result_df_model_previous(test_sentences, test_tags, model_14, word2index, index2tag, max_len)\n",
    "result_df_model_14.to_csv(\"result_df_model_14.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_15 = result_df_model_previous(test_sentences, test_tags, model_15, word2index, index2tag, max_len)\n",
    "result_df_model_15.to_csv(\"result_df_model_15.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_16 = result_df_model_previous(test_sentences, test_tags, model_16, word2index, index2tag, max_len)\n",
    "result_df_model_16.to_csv(\"result_df_model_16.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_17 = result_df_model_previous(test_sentences, test_tags, model_17, word2index, index2tag, max_len)\n",
    "result_df_model_17.to_csv(\"result_df_model_17.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_18 = result_df_model_previous(test_sentences, test_tags, model_18, word2index, index2tag, max_len)\n",
    "result_df_model_18.to_csv(\"result_df_model_18.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_19 = result_df_model_previous(test_sentences, test_tags, model_19, word2index, index2tag, max_len)\n",
    "result_df_model_19.to_csv(\"result_df_model_19.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_20 = result_df_model_previous(test_sentences, test_tags, model_20, word2index, index2tag, max_len)\n",
    "result_df_model_20.to_csv(\"result_df_model_20.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_21 = result_df_model_previous(test_sentences, test_tags, model_21, word2index, index2tag, max_len)\n",
    "result_df_model_21.to_csv(\"result_df_model_21.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_22 = result_df_model_previous(test_sentences, test_tags, model_22, word2index, index2tag, max_len)\n",
    "result_df_model_22.to_csv(\"result_df_model_22.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_23 = result_df_model_previous(test_sentences, test_tags, model_23, word2index, index2tag, max_len)\n",
    "result_df_model_23.to_csv(\"result_df_model_23.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_24 = result_df_model_previous(test_sentences, test_tags, model_24, word2index, index2tag, max_len)\n",
    "result_df_model_24.to_csv(\"result_df_model_24.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_25 = result_df_model_previous(test_sentences, test_tags, model_25, word2index, index2tag, max_len)\n",
    "result_df_model_25.to_csv(\"result_df_model_25.csv\", encoding='utf-8')\n",
    "\n",
    "result_df_model_26 = result_df_model_previous(test_sentences, test_tags, model_26, word2index, index2tag, max_len)\n",
    "result_df_model_26.to_csv(\"result_df_model_26.csv\", encoding='utf-8')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_00.save(\"model_00\")\n",
    "model_01.save(\"model_01\")\n",
    "model_02.save(\"model_02\")\n",
    "model_03.save(\"model_03\")\n",
    "model_04.save(\"model_04\")\n",
    "model_05.save(\"model_05\")\n",
    "model_06.save(\"model_06\")\n",
    "model_07.save(\"model_07\")\n",
    "model_08.save(\"model_08\")\n",
    "model_09.save(\"model_09\")\n",
    "model_10.save(\"model_10\")\n",
    "model_11.save(\"model_11\")\n",
    "model_12.save(\"model_12\")\n",
    "model_13.save(\"model_13\")\n",
    "model_14.save(\"model_14\")\n",
    "model_15.save(\"model_15\")\n",
    "model_16.save(\"model_16\")\n",
    "model_17.save(\"model_17\")\n",
    "model_18.save(\"model_18\")\n",
    "model_19.save(\"model_19\")\n",
    "model_20.save(\"model_20\")\n",
    "model_21.save(\"model_21\")\n",
    "model_22.save(\"model_22\")\n",
    "model_23.save(\"model_23\")\n",
    "model_24.save(\"model_24\")\n",
    "model_25.save(\"model_25\")\n",
    "model_26.save(\"model_26\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models - TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model_00.save(\"model_00.tf\")\n",
    "model_01.save(\"model_01.tf\")\n",
    "model_02.save(\"model_02.tf\")\n",
    "model_03.save(\"model_03.tf\")\n",
    "model_04.save(\"model_04.tf\")\n",
    "model_05.save(\"model_05.tf\")\n",
    "model_06.save(\"model_06.tf\")\n",
    "model_07.save(\"model_07.tf\")\n",
    "model_08.save(\"model_08.tf\")\n",
    "model_09.save(\"model_09.tf\")\n",
    "model_10.save(\"model_10.tf\")\n",
    "model_11.save(\"model_11.tf\")\n",
    "model_12.save(\"model_12.tf\")\n",
    "model_13.save(\"model_13.tf\")\n",
    "model_14.save(\"model_14.tf\")\n",
    "model_15.save(\"model_15.tf\")\n",
    "model_16.save(\"model_16.tf\")\n",
    "model_17.save(\"model_17.tf\")\n",
    "model_18.save(\"model_18.tf\")\n",
    "model_19.save(\"model_19.tf\")\n",
    "model_20.save(\"model_20.tf\")\n",
    "model_21.save(\"model_21.tf\")\n",
    "model_22.save(\"model_22.tf\")\n",
    "model_23.save(\"model_23.tf\")\n",
    "model_24.save(\"model_24.tf\")\n",
    "model_25.save(\"model_25.tf\")\n",
    "model_26.save(\"model_26.tf\")\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models - H5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model_00.save(\"model_00.h5\")\n",
    "model_01.save(\"model_01.h5\")\n",
    "model_02.save(\"model_02.h5\")\n",
    "model_03.save(\"model_03.h5\")\n",
    "model_04.save(\"model_04.h5\")\n",
    "model_05.save(\"model_05.h5\")\n",
    "model_06.save(\"model_06.h5\")\n",
    "model_07.save(\"model_07.h5\")\n",
    "model_08.save(\"model_08.h5\")\n",
    "model_09.save(\"model_09.h5\")\n",
    "model_10.save(\"model_10.h5\")\n",
    "model_11.save(\"model_11.h5\")\n",
    "model_12.save(\"model_12.h5\")\n",
    "model_13.save(\"model_13.h5\")\n",
    "model_14.save(\"model_14.h5\")\n",
    "model_15.save(\"model_15.h5\")\n",
    "model_16.save(\"model_16.h5\")\n",
    "model_17.save(\"model_17.h5\")\n",
    "model_18.save(\"model_18.h5\")\n",
    "model_19.save(\"model_19.h5\")\n",
    "model_20.save(\"model_20.h5\")\n",
    "model_21.save(\"model_21.h5\")\n",
    "model_22.save(\"model_22.h5\")\n",
    "model_23.save(\"model_23.h5\")\n",
    "model_24.save(\"model_24.h5\")\n",
    "model_25.save(\"model_25.h5\")\n",
    "model_26.save(\"model_26.h5\")\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tuples(data):\n",
    "    iterator = zip(data[\"word\"].values.tolist(),\n",
    "                  data[\"tag\"].values.tolist(),\n",
    "                  data[\"tag_pred\"].values.tolist())\n",
    "    return [(word, tag, tag_pred) for word, tag, tag_pred in iterator]\n",
    "\n",
    "def tuple_2_list(sentences):\n",
    "    texts = [[word[0] for word in sentence] for sentence in sentences]\n",
    "    tags = [[word[1] for word in sentence] for sentence in sentences]\n",
    "    tags_pred = [[word[2] for word in sentence] for sentence in sentences]\n",
    "\n",
    "    return texts, tags, tags_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = ['O',\n",
    "'ACH',\n",
    "'ATE',\n",
    "'BORDAS',\n",
    "'CAL',\n",
    "'LOC',\n",
    "'TAM',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()+\"/results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = os.listdir(current_directory)\n",
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = []\n",
    "results_by_model = []\n",
    "results_by_model_by_tag = []\n",
    "\n",
    "for filename in filenames:\n",
    "    f = os.path.join(current_directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        print(f)\n",
    "\n",
    "    data = pd.read_csv(f, index_col=0)\n",
    "\n",
    "    sentences = data.groupby(\"report\").apply(to_tuples).tolist()\n",
    "    texts, tags, tags_pred = tuple_2_list(sentences)\n",
    "    \n",
    "    # Avaliação\n",
    "    evaluator = Evaluator(tags, tags_pred, tags=all_tags, loader=\"list\")\n",
    "\n",
    "    # Returns overall metrics and metrics for each tag\n",
    "    results, results_by_tag = evaluator.evaluate()\n",
    " \n",
    "    model_name.append(filename)\n",
    "    results_by_model.append(results)\n",
    "    results_by_model_by_tag.append(results_by_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_model_20_df = pd.DataFrame(results_by_model[20])\n",
    "\n",
    "metrics_localizacao_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Localização\"])\n",
    "\n",
    "metrics_calcificacao_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Calcificação\"])\n",
    "\n",
    "metrics_achado_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Achado\"])\n",
    "\n",
    "metrics_bordas_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Bordas\"])\n",
    "\n",
    "metrics_atenuacao_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Atenuação\"])\n",
    "\n",
    "metrics_tamanho_model_20_df = pd.DataFrame(results_by_model_by_tag[20][\"Tamanho\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_model_20_df.to_csv(\"metrics_model_20.csv\")\n",
    "metrics_localizacao_model_20_df.to_csv(\"metrics_localizacao_model_20.csv\")\n",
    "metrics_calcificacao_model_20_df.to_csv(\"metrics_calcificacao_model_20.csv\")\n",
    "metrics_achado_model_20_df.to_csv(\"metrics_achado_model_20.csv\")\n",
    "metrics_bordas_model_20_df.to_csv(\"metrics_bordas_model_20.csv\")\n",
    "metrics_atenuacao_model_20_df.to_csv(\"metrics_atenuacao_model_20.csv\")\n",
    "metrics_tamanho_model_20_df.to_csv(\"metrics_tamanho_model_20.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': {'ent_type': {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0, 'f1': 0}, 'partial': {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0, 'f1': 0}, 'strict': {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0, 'f1': 0}, 'exact': {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'possible': 0, 'actual': 0, 'precision': 0, 'recall': 0, 'f1': 0}}, 'Achado': {'ent_type': {'correct': 120, 'incorrect': 0, 'partial': 0, 'missed': 15, 'spurious': 6, 'possible': 135, 'actual': 126, 'precision': 0.9523809523809523, 'recall': 0.8888888888888888, 'f1': 0.9195402298850575}, 'partial': {'correct': 116, 'incorrect': 0, 'partial': 4, 'missed': 15, 'spurious': 6, 'possible': 135, 'actual': 126, 'precision': 0.9365079365079365, 'recall': 0.8740740740740741, 'f1': 0.9042145593869733}, 'strict': {'correct': 116, 'incorrect': 4, 'partial': 0, 'missed': 15, 'spurious': 6, 'possible': 135, 'actual': 126, 'precision': 0.9206349206349206, 'recall': 0.8592592592592593, 'f1': 0.8888888888888888}, 'exact': {'correct': 116, 'incorrect': 4, 'partial': 0, 'missed': 15, 'spurious': 6, 'possible': 135, 'actual': 126, 'precision': 0.9206349206349206, 'recall': 0.8592592592592593, 'f1': 0.8888888888888888}}, 'Atenuação': {'ent_type': {'correct': 20, 'incorrect': 1, 'partial': 0, 'missed': 4, 'spurious': 4, 'possible': 25, 'actual': 25, 'precision': 0.8, 'recall': 0.8, 'f1': 0.8000000000000002}, 'partial': {'correct': 18, 'incorrect': 0, 'partial': 3, 'missed': 4, 'spurious': 4, 'possible': 25, 'actual': 25, 'precision': 0.78, 'recall': 0.78, 'f1': 0.78}, 'strict': {'correct': 18, 'incorrect': 3, 'partial': 0, 'missed': 4, 'spurious': 4, 'possible': 25, 'actual': 25, 'precision': 0.72, 'recall': 0.72, 'f1': 0.72}, 'exact': {'correct': 18, 'incorrect': 3, 'partial': 0, 'missed': 4, 'spurious': 4, 'possible': 25, 'actual': 25, 'precision': 0.72, 'recall': 0.72, 'f1': 0.72}}, 'Bordas': {'ent_type': {'correct': 6, 'incorrect': 1, 'partial': 0, 'missed': 2, 'spurious': 2, 'possible': 9, 'actual': 9, 'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1': 0.6666666666666666}, 'partial': {'correct': 5, 'incorrect': 0, 'partial': 2, 'missed': 2, 'spurious': 2, 'possible': 9, 'actual': 9, 'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1': 0.6666666666666666}, 'strict': {'correct': 5, 'incorrect': 2, 'partial': 0, 'missed': 2, 'spurious': 2, 'possible': 9, 'actual': 9, 'precision': 0.5555555555555556, 'recall': 0.5555555555555556, 'f1': 0.5555555555555556}, 'exact': {'correct': 5, 'incorrect': 2, 'partial': 0, 'missed': 2, 'spurious': 2, 'possible': 9, 'actual': 9, 'precision': 0.5555555555555556, 'recall': 0.5555555555555556, 'f1': 0.5555555555555556}}, 'Calcificação': {'ent_type': {'correct': 71, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 5, 'possible': 71, 'actual': 76, 'precision': 0.9342105263157895, 'recall': 1.0, 'f1': 0.9659863945578232}, 'partial': {'correct': 69, 'incorrect': 0, 'partial': 2, 'missed': 0, 'spurious': 5, 'possible': 71, 'actual': 76, 'precision': 0.9210526315789473, 'recall': 0.9859154929577465, 'f1': 0.9523809523809524}, 'strict': {'correct': 69, 'incorrect': 2, 'partial': 0, 'missed': 0, 'spurious': 5, 'possible': 71, 'actual': 76, 'precision': 0.9078947368421053, 'recall': 0.971830985915493, 'f1': 0.9387755102040817}, 'exact': {'correct': 69, 'incorrect': 2, 'partial': 0, 'missed': 0, 'spurious': 5, 'possible': 71, 'actual': 76, 'precision': 0.9078947368421053, 'recall': 0.971830985915493, 'f1': 0.9387755102040817}}, 'Localização': {'ent_type': {'correct': 125, 'incorrect': 1, 'partial': 0, 'missed': 17, 'spurious': 13, 'possible': 143, 'actual': 139, 'precision': 0.8992805755395683, 'recall': 0.8741258741258742, 'f1': 0.8865248226950354}, 'partial': {'correct': 114, 'incorrect': 0, 'partial': 12, 'missed': 17, 'spurious': 13, 'possible': 143, 'actual': 139, 'precision': 0.8633093525179856, 'recall': 0.8391608391608392, 'f1': 0.851063829787234}, 'strict': {'correct': 113, 'incorrect': 13, 'partial': 0, 'missed': 17, 'spurious': 13, 'possible': 143, 'actual': 139, 'precision': 0.8129496402877698, 'recall': 0.7902097902097902, 'f1': 0.8014184397163121}, 'exact': {'correct': 114, 'incorrect': 12, 'partial': 0, 'missed': 17, 'spurious': 13, 'possible': 143, 'actual': 139, 'precision': 0.8201438848920863, 'recall': 0.7972027972027972, 'f1': 0.8085106382978723}}, 'Tamanho': {'ent_type': {'correct': 114, 'incorrect': 2, 'partial': 0, 'missed': 7, 'spurious': 16, 'possible': 123, 'actual': 132, 'precision': 0.8636363636363636, 'recall': 0.926829268292683, 'f1': 0.8941176470588236}, 'partial': {'correct': 114, 'incorrect': 0, 'partial': 2, 'missed': 7, 'spurious': 16, 'possible': 123, 'actual': 132, 'precision': 0.8712121212121212, 'recall': 0.9349593495934959, 'f1': 0.9019607843137255}, 'strict': {'correct': 112, 'incorrect': 4, 'partial': 0, 'missed': 7, 'spurious': 16, 'possible': 123, 'actual': 132, 'precision': 0.8484848484848485, 'recall': 0.9105691056910569, 'f1': 0.8784313725490196}, 'exact': {'correct': 114, 'incorrect': 2, 'partial': 0, 'missed': 7, 'spurious': 16, 'possible': 123, 'actual': 132, 'precision': 0.8636363636363636, 'recall': 0.926829268292683, 'f1': 0.8941176470588236}}}\n"
     ]
    }
   ],
   "source": [
    "best_model_tags_ent_type = []\n",
    "best_model_tags_partial = []\n",
    "best_model_tags_strict = []\n",
    "best_model_tags_exact = []\n",
    "\n",
    "best_model = results_by_model_by_tag[20]\n",
    "\n",
    "print(best_model)\n",
    "\n",
    "for entity in all_tags:\n",
    "    tag_ent_type = []\n",
    "    tag_partial = []\n",
    "    tag_strict = []\n",
    "    tag_exact = []\n",
    "\n",
    "    entity = str(entity)\n",
    "    # entity_type\n",
    "    ent_precision = round(best_model[entity][\"ent_type\"][\"precision\"], 2)\n",
    "    ent_recall = round(best_model[entity][\"ent_type\"][\"recall\"], 2)\n",
    "    ent_f1 = round(best_model[entity][\"ent_type\"][\"f1\"], 2)\n",
    "    tag_ent_type.append(ent_precision)\n",
    "    tag_ent_type.append(ent_recall)\n",
    "    tag_ent_type.append(ent_f1)\n",
    "\n",
    "    # partial\n",
    "    partial_precision = round(best_model[entity][\"partial\"][\"precision\"], 2)\n",
    "    partial_recall = round(best_model[entity][\"partial\"][\"recall\"], 2)\n",
    "    partial_f1 = round(best_model[entity][\"partial\"][\"f1\"], 2)\n",
    "    tag_partial.append(partial_precision)\n",
    "    tag_partial.append(partial_recall)\n",
    "    tag_partial.append(partial_f1)\n",
    "\n",
    "    # strict\n",
    "    strict_precision = round(best_model[entity][\"strict\"][\"precision\"], 2)\n",
    "    strict_recall = round(best_model[entity][\"strict\"][\"recall\"], 2)\n",
    "    strict_f1 = round(best_model[entity][\"strict\"][\"f1\"], 2)\n",
    "    tag_strict.append(strict_precision)\n",
    "    tag_strict.append(strict_recall)\n",
    "    tag_strict.append(strict_f1)\n",
    "\n",
    "    # exact\n",
    "    exact_precision = round(best_model[entity][\"exact\"][\"precision\"], 2)\n",
    "    exact_recall = round(best_model[entity][\"exact\"][\"recall\"], 2)\n",
    "    exact_f1 = round(best_model[entity][\"exact\"][\"f1\"], 2)\n",
    "    tag_exact.append(exact_precision)\n",
    "    tag_exact.append(exact_recall)\n",
    "    tag_exact.append(exact_f1)\n",
    "\n",
    "    best_model_tags_ent_type.append(tag_ent_type)\n",
    "    best_model_tags_partial.append(tag_partial)\n",
    "    best_model_tags_strict.append(tag_strict)\n",
    "    best_model_tags_exact.append(tag_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_tags_ent_type_np = np.array(best_model_tags_ent_type)\n",
    "best_model_tags_partial_np = np.array(best_model_tags_partial)\n",
    "best_model_tags_strict_np = np.array(best_model_tags_strict)\n",
    "best_model_tags_exact_np = np.array(best_model_tags_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_type_results_best_model_df = pd.DataFrame(best_model_tags_ent_type_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "partial_results_best_model_df = pd.DataFrame(best_model_tags_partial_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "strict_results_best_model_df = pd.DataFrame(best_model_tags_strict_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "exact_results_best_model_df = pd.DataFrame(best_model_tags_exact_np, columns = [\"Precision\", \"Recall\", \"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_type_results_best_model_df.to_csv(\"ent_type_results_best_model_df.csv\", encoding=\"utf-8\")\n",
    "partial_results_best_model_df.to_csv(\"partial_results_best_model_df.csv\", encoding=\"utf-8\")\n",
    "strict_results_best_model_df.to_csv(\"strict_results_best_model_df.csv\", encoding=\"utf-8\")\n",
    "exact_results_best_model_df.to_csv(\"exact_results_best_model_df.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliar os Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ent_type = []\n",
    "models_partial = []\n",
    "models_strict = []\n",
    "models_exact = []\n",
    "\n",
    "for result_model in results_by_model:\n",
    "    ent_type = []\n",
    "    partial = []\n",
    "    strict = []\n",
    "    exact = []\n",
    "\n",
    "    # entity_type\n",
    "    ent_precision = round(result_model[\"ent_type\"][\"precision\"], 2)\n",
    "    ent_recall = round(result_model[\"ent_type\"][\"recall\"], 2)\n",
    "    ent_f1 = round(result_model[\"ent_type\"][\"f1\"], 2)\n",
    "    ent_type.append(ent_precision)\n",
    "    ent_type.append(ent_recall)\n",
    "    ent_type.append(ent_f1)\n",
    "\n",
    "    # partial\n",
    "    partial_precision = round(result_model[\"partial\"][\"precision\"], 2)\n",
    "    partial_recall = round(result_model[\"partial\"][\"recall\"], 2)\n",
    "    partial_f1 = round(result_model[\"partial\"][\"f1\"], 2)\n",
    "    partial.append(partial_precision)\n",
    "    partial.append(partial_recall)\n",
    "    partial.append(partial_f1)\n",
    "\n",
    "    # strict\n",
    "    strict_precision = round(result_model[\"strict\"][\"precision\"], 2)\n",
    "    strict_recall = round(result_model[\"strict\"][\"recall\"], 2)\n",
    "    strict_f1 = round(result_model[\"strict\"][\"f1\"], 2)\n",
    "    strict.append(strict_precision)\n",
    "    strict.append(strict_recall)\n",
    "    strict.append(strict_f1)\n",
    "\n",
    "    # exact\n",
    "    exact_precision = round(result_model[\"exact\"][\"precision\"], 2)\n",
    "    exact_recall = round(result_model[\"exact\"][\"recall\"], 2)\n",
    "    exact_f1 = round(result_model[\"exact\"][\"f1\"], 2)\n",
    "    exact.append(exact_precision)\n",
    "    exact.append(exact_recall)\n",
    "    exact.append(exact_f1)\n",
    "\n",
    "\n",
    "    models_ent_type.append(ent_type)\n",
    "    models_partial.append(partial)\n",
    "    models_strict.append(strict)\n",
    "    models_exact.append(exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ent_type_np = np.array(models_ent_type)\n",
    "models_partial_np = np.array(models_partial)\n",
    "models_strict_np = np.array(models_strict)\n",
    "models_exact_np = np.array(models_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_type_results_by_model_df = pd.DataFrame(models_ent_type_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "partial_results_by_model_df = pd.DataFrame(models_partial_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "strict_results_by_model_df = pd.DataFrame(models_strict_np, columns = [\"Precision\", \"Recall\", \"F1\"])\n",
    "exact_results_by_model_df = pd.DataFrame(models_exact_np, columns = [\"Precision\", \"Recall\", \"F1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_type_results_by_model_df.to_csv(\"ent_type_results_by_model_df.csv\", encoding=\"utf-8\")\n",
    "partial_results_by_model_df.to_csv(\"partial_results_by_model_df.csv\", encoding=\"utf-8\")\n",
    "strict_results_by_model_df.to_csv(\"strict_results_by_model_df.csv\", encoding=\"utf-8\")\n",
    "exact_results_by_model_df.to_csv(\"exact_results_by_model_df.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = 'c:\\\\Users\\\\tarci\\\\Desktop\\\\modelos_mestrado\\\\bilstm_antigo\\\\models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = DENSE_EMBEDDING_LIST[0]\n",
    "lstm_unit = LSTM_UNITS_LIST[0]\n",
    "batch_size = BATCH_SIZE_LIST[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test =  bilstm_crf(maxlen = max_len, n_tags = n_tags, lstm_units = lstm_unit, embedding_dim = embed_dim, n_words = n_words, mask_zero = True)\n",
    "\n",
    "# compile model\n",
    "model_test.compile(optimizer = Adam(learning_rate = lr), loss = model_test.layers[-1].loss, metrics = model_test.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\n",
    "model_test.fit(X_train, y_train, epochs = 1, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.save(\"model_teste_load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessário passar essas funções "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_accuracy(y_true, y_pred):\n",
    "    mask = K.cast(K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
    "    shape = tf.shape(y_pred)\n",
    "    sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "    y_pred, _ = crf_decode(y_pred, K.zeros_like(y_pred), sequence_lengths)\n",
    "    if K.ndim(y_true) == K.ndim(y_pred) - 1:\n",
    "        y_true = K.expand_dims(y_true, K.ndim(y_pred) - 1)\n",
    "    y_pred = K.cast(y_pred, 'int32')\n",
    "    y_true = K.cast(y_true, 'int32')\n",
    "    corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "    return K.sum(corrects * mask) / K.sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_loss(y_true, y_pred):\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=CRF(dtype='float32').dtype)\n",
    "    log_likelihood, _ = tf.keras.layers.CRF(dtype='float32')(y_pred, y_true)\n",
    "    return tf.reduce_mean(-log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('model_teste_load', custom_objects={'CRF': CRF, 'viterbi_accuracy': viterbi_accuracy, 'crf_loss': crf_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "loaded_model.compile(optimizer = Adam(learning_rate = lr), loss = loaded_model.layers[-1].loss, metrics = loaded_model.layers[-1].accuracy)\n",
    "\n",
    "# train model\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 5, verbose = 1)\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0, patience = 5, verbose = 1)\n",
    "\n",
    "loaded_model.fit(X_train, y_train, epochs = 1, initial_epoch = 0, callbacks = [early_stopping, reduce_lr], verbose = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = []\n",
    "results_by_model = []\n",
    "results_by_model_by_tag = []\n",
    "\n",
    "for filename in filenames:\n",
    "    f = os.path.join(current_directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        print(f)\n",
    "\n",
    "    data = pd.read_csv(f, index_col=0)\n",
    "    sentences = data.groupby(\"report\").apply(to_tuples).tolist()\n",
    "    texts, tags, tags_pred = tuple_2_list(sentences)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(filename)\n",
    "    result_dict = classification_report(tags, tags_pred, mode=\"strict\", scheme=IOB2, zero_division=False)\n",
    "    print(result_dict)\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    if filename == \"result_df_model_13.csv\":\n",
    "        best_result = result_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = json.loads(best_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bilstmcrf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
